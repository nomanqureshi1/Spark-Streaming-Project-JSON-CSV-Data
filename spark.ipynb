{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb381a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d538768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/04 15:16:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JSON File Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82a9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"event_time\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d011b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Users/nomantahir/Desktop/Project-Spark_streaming/input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de48a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"/Users/nomantahir/Desktop/Project-Spark_streaming/output\") \\\n",
    "    .option(\"checkpointLocation\", \"/Users/nomantahir/Desktop/Project-Spark_streaming/checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b9174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "|_c0|_c1|                _c2|\n",
      "+---+---+-------------------+\n",
      "| 10|897|2026-01-04 14:34:24|\n",
      "| 10|607|2026-01-04 14:34:34|\n",
      "|  2|591|2026-01-04 14:26:29|\n",
      "|  5|295|2026-01-04 14:26:31|\n",
      "|  2|271|2026-01-04 14:26:33|\n",
      "|  3|882|2026-01-04 14:26:35|\n",
      "|  4|341|2026-01-04 14:26:37|\n",
      "|  5|142|2026-01-04 14:34:02|\n",
      "|  8|865|2026-01-04 14:34:04|\n",
      "|  4|308|2026-01-04 14:34:06|\n",
      "|  3|754|2026-01-04 14:34:08|\n",
      "|  8|248|2026-01-04 14:34:10|\n",
      "|  4|959|2026-01-04 14:34:12|\n",
      "|  5|461|2026-01-04 14:34:14|\n",
      "|  4|348|2026-01-04 14:34:16|\n",
      "|  1|771|2026-01-04 14:34:18|\n",
      "|  9|522|2026-01-04 14:34:20|\n",
      "|  6|967|2026-01-04 14:34:22|\n",
      "|  3|130|2026-01-04 14:34:26|\n",
      "|  7|331|2026-01-04 14:34:28|\n",
      "|  6|629|2026-01-04 14:34:30|\n",
      "|  7|579|2026-01-04 14:34:32|\n",
      "|  4|531|2026-01-04 14:34:36|\n",
      "|  3|102|2026-01-04 14:34:38|\n",
      "|  8|291|2026-01-04 14:34:40|\n",
      "|  2|749|2026-01-04 14:34:42|\n",
      "|  4|630|2026-01-04 14:34:44|\n",
      "|  8|886|2026-01-04 14:34:46|\n",
      "|  3|963|2026-01-04 14:34:48|\n",
      "|  9|577|2026-01-04 14:34:50|\n",
      "|  1|299|2026-01-04 14:34:52|\n",
      "|  4|263|2026-01-04 14:34:54|\n",
      "|  2|901|2026-01-04 14:34:56|\n",
      "|  4|990|2026-01-04 14:34:58|\n",
      "| 10|727|2026-01-04 15:03:17|\n",
      "| 10|249|2026-01-04 15:02:45|\n",
      "| 10|425|2026-01-04 15:08:34|\n",
      "| 10|263|2026-01-04 15:08:42|\n",
      "| 10|398|2026-01-04 15:03:37|\n",
      "| 10|811|2026-01-04 15:03:01|\n",
      "|  7|215|2026-01-04 15:03:03|\n",
      "|  2|996|2026-01-04 15:03:07|\n",
      "|  8|305|2026-01-04 15:02:49|\n",
      "|  4|287|2026-01-04 15:08:52|\n",
      "|  2|862|2026-01-04 15:03:43|\n",
      "|  5|629|2026-01-04 15:03:33|\n",
      "|  5|226|2026-01-04 15:03:09|\n",
      "|  2|423|2026-01-04 15:03:23|\n",
      "|  7|694|2026-01-04 15:08:26|\n",
      "|  4|271|2026-01-04 15:08:44|\n",
      "|  2|523|2026-01-04 15:08:28|\n",
      "|  2|412|2026-01-04 15:02:53|\n",
      "|  6|950|2026-01-04 15:02:57|\n",
      "|  4|493|2026-01-04 15:08:48|\n",
      "|  5|790|2026-01-04 15:02:59|\n",
      "|  9|990|2026-01-04 15:03:05|\n",
      "|  1|129|2026-01-04 15:03:45|\n",
      "|  7|203|2026-01-04 15:03:13|\n",
      "|  3|118|2026-01-04 15:08:24|\n",
      "|  5|539|2026-01-04 15:08:50|\n",
      "|  7|886|2026-01-04 15:03:35|\n",
      "|  5|491|2026-01-04 15:02:55|\n",
      "|  7|990|2026-01-04 15:03:21|\n",
      "|  2|887|2026-01-04 15:02:43|\n",
      "|  9|157|2026-01-04 15:03:11|\n",
      "|  7|824|2026-01-04 15:03:29|\n",
      "|  2|630|2026-01-04 15:03:31|\n",
      "|  9|849|2026-01-04 15:08:32|\n",
      "|  1|200|2026-01-04 15:08:38|\n",
      "|  7|887|2026-01-04 15:03:19|\n",
      "|  6|943|2026-01-04 15:08:40|\n",
      "|  3|759|2026-01-04 15:08:20|\n",
      "|  8|658|2026-01-04 15:03:25|\n",
      "|  9|384|2026-01-04 15:03:41|\n",
      "|  2|281|2026-01-04 15:08:22|\n",
      "|  6|993|2026-01-04 15:08:30|\n",
      "|  9|998|2026-01-04 15:03:15|\n",
      "|  3|625|2026-01-04 15:03:27|\n",
      "|  8|940|2026-01-04 15:02:47|\n",
      "|  5|377|2026-01-04 15:02:51|\n",
      "|  5|350|2026-01-04 15:03:39|\n",
      "|  7|241|2026-01-04 15:08:46|\n",
      "|  4|525|2026-01-04 15:08:36|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all1 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Users/nomantahir/Desktop/Project-Spark_streaming/output/*\")\n",
    "\n",
    "df_all1.show(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
